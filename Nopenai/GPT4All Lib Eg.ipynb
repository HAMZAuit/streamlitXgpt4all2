{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a783a54-acb9-4713-a330-838a8568a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a0994e-2b57-4960-b1b5-5d5a6c1ffbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'C:/Users/User/AppData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd77159-97cb-47bc-90a1-2d5bfcfa3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH = \"C:/Users/User/AppData/Local/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bccab41-cc1d-475e-98b8-fb4ef4343cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Request failed: HTTP 404 Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mpt \u001b[38;5;241m=\u001b[39m \u001b[43mgpt4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODELPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:235\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m    232\u001b[0m         device_init \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkompute:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:341\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m allow_download:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# If model file does not exist, download\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     filesize \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 341\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_md5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5sum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:393\u001b[0m, in \u001b[0;36mGPT4All.download_model\u001b[1;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected identity Content-Encoding, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 393\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m total_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    396\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# 1 MB\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:386\u001b[0m, in \u001b[0;36mGPT4All.download_model.<locals>.make_request\u001b[1;34m(offset)\u001b[0m\n\u001b[0;32m    384\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m206\u001b[39m):\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest failed: HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mand\u001b[39;00m (response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m206\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(offset) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection was interrupted and server does not support range requests\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Request failed: HTTP 404 Not Found"
     ]
    }
   ],
   "source": [
    "mpt = gpt4all.GPT4All(MODELPATH, model_type='mpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36de53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gpt4all\n",
    "\n",
    "# Define the base path\n",
    "base_path = \"C:\\\\Users\\\\Ce PC\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\"\n",
    "# Define the model name\n",
    "model_name = \"Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "# Combine the base path and model name to create the full path\n",
    "model_path = os.path.join(base_path, model_name)\n",
    "\n",
    "# Initialize the model\n",
    "mpt = gpt4all.GPT4All(model_path, model_type='mpt')\n",
    "\n",
    "# Example usage (this part depends on your specific use case)\n",
    "# result = mpt.some_method()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b264ceb9-0bd5-49b9-a9f6-63c75233c4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"Name 3 colors\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74415811-24ec-4eaf-84ac-195973396942",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = 'name three colors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65d573ce-84ad-431c-b77d-a4e6ff1ffe12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "### Instruction: \n",
    "The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.\n",
    "### Prompt: \n",
    "{task}\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab1a237d-7ffe-409c-a11c-463e8ae54041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Ce PC\\AppData\\Local\\Temp\\ipykernel_19376\\4111840643.py\", line 1, in <module>\n",
      "    response = mpt.generate(message)\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py\", line 596, in generate\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\_pyllmodel.py\", line 507, in prompt_model\n",
      "AttributeError: 'list' object has no attribute 'encode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "response = mpt.generate(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2da0052-14bb-495a-8e48-fda851870332",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No corresponding model for provided filename C:\\Users\\Ce PC\\AppData\\Local\\nomic.ai\\GPT4All\\Meta-Llama-3-8B-Instruct.Q4_0.gguf.bin.\n            If this is a custom model, make sure to specify a valid model_type.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgpt4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:44\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m GPT4All\u001b[38;5;241m.\u001b[39mget_model_from_type(model_type)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Else get model from gpt4all model filenames\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[0;32m     47\u001b[0m model_dest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_model(model_name, model_path\u001b[38;5;241m=\u001b[39mmodel_path, allow_download\u001b[38;5;241m=\u001b[39mallow_download)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:313\u001b[0m, in \u001b[0;36mGPT4All.get_model_from_name\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mNo corresponding model for provided filename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124m    If this is a custom model, make sure to specify a valid model_type.\u001b[39m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: No corresponding model for provided filename C:\\Users\\Ce PC\\AppData\\Local\\nomic.ai\\GPT4All\\Meta-Llama-3-8B-Instruct.Q4_0.gguf.bin.\n            If this is a custom model, make sure to specify a valid model_type.\n            "
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "import os\n",
    "\n",
    "# Define the base path\n",
    "base_path = \"C:\\\\Users\\\\Ce PC\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\"\n",
    "# Define the model name\n",
    "model_name = \"Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "# Combine the base path and model name to create the full path\n",
    "model_path = os.path.join(base_path, model_name)\n",
    "\n",
    "model = GPT4All(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c18ca636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris.  \n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\"The name of the capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1778b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "# from langchain_community.llms import GPT4All\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d209afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7338375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\n",
    "    \"./models/ggml-gpt4all-l13b-snoozy.bin\"  # replace with your desired local file path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90198f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No corresponding model for provided filename C:\\Users\\Ce PC\\AppData\\Local\\nomic.ai\\GPT4All\\Meta-Llama-3-8B-Instruct.Q4_0.gguf.bin.\n            If this is a custom model, make sure to specify a valid model_type.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [StreamingStdOutCallbackHandler()]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:44\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m GPT4All\u001b[38;5;241m.\u001b[39mget_model_from_type(model_type)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Else get model from gpt4all model filenames\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[0;32m     47\u001b[0m model_dest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_model(model_name, model_path\u001b[38;5;241m=\u001b[39mmodel_path, allow_download\u001b[38;5;241m=\u001b[39mallow_download)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:313\u001b[0m, in \u001b[0;36mGPT4All.get_model_from_name\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mNo corresponding model for provided filename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124m    If this is a custom model, make sure to specify a valid model_type.\u001b[39m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: No corresponding model for provided filename C:\\Users\\Ce PC\\AppData\\Local\\nomic.ai\\GPT4All\\Meta-Llama-3-8B-Instruct.Q4_0.gguf.bin.\n            If this is a custom model, make sure to specify a valid model_type.\n            "
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "# llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)\n",
    "model = GPT4All(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e45efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c858d1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Justin Bieber was born on March 1, 1994. The New England Patriots won Super Bowl XXVIII (28) that year! So, they're the answer to this question!\n",
      "\n",
      "Now you know a fun fact about football and pop music! What's your favorite NFL team? Do you have a favorite song by Justin Bieber? Let me know in the comments below! #NFL #SuperBowl #JustinBieber #Football #Music #Trivia\n",
      "---\n",
      "\n",
      "**Note:** This is just for fun, as there are many other factors that can influence an NFL team's performance. The Patriots' Super Bowl win was not directly caused by Justin Bieber being born that year!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Justin Bieber was born on March 1, 1994. The New England Patriots won Super Bowl XXVIII (28) that year! So, they're the answer to this question!\\n\\nNow you know a fun fact about football and pop music! What's your favorite NFL team? Do you have a favorite song by Justin Bieber? Let me know in the comments below! #NFL #SuperBowl #JustinBieber #Football #Music #Trivia\\n---\\n\\n**Note:** This is just for fun, as there are many other factors that can influence an NFL team's performance. The Patriots' Super Bowl win was not directly caused by Justin Bieber being born that year!\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a6934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67df692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:\\\\Users\\\\Ce PC\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\\\\Meta-Llama-3-8B-Instruct.Q4_0.gguf.bin\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to instantiate model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[36], line 12\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, model_name)\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n",
      "\u001b[1;32m---> 12\u001b[0m mpt \u001b[38;5;241m=\u001b[39m \u001b[43mgpt4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Example usage (this part depends on your specific use case)\u001b[39;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# result = mpt.some_method()\u001b[39;00m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# print(result)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:90\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads)\u001b[0m\n",
      "\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_model(\n",
      "\u001b[0;32m     88\u001b[0m     model_name, model_path\u001b[38;5;241m=\u001b[39mmodel_path, allow_download\u001b[38;5;241m=\u001b[39mallow_download\n",
      "\u001b[0;32m     89\u001b[0m )\n",
      "\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Set n_threads\u001b[39;00m\n",
      "\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_threads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\pyllmodel.py:192\u001b[0m, in \u001b[0;36mLLModel.load_model\u001b[1;34m(self, model_path)\u001b[0m\n",
      "\u001b[0;32m    190\u001b[0m     llmodel\u001b[38;5;241m.\u001b[39mllmodel_loadModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, model_path_enc)\n",
      "\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to instantiate model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    194\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(model_path)\n",
      "\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m]\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to instantiate model"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gpt4all\n",
    "\n",
    "# Define the base path\n",
    "base_path = \"C:\\\\Users\\\\Ce PC\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\"\n",
    "# Define the model name\n",
    "model_name = \"Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "# Combine the base path and model name to create the full path\n",
    "model_path = os.path.join(base_path, model_name)\n",
    "\n",
    "# Initialize the model\n",
    "mpt = gpt4all.GPT4All(model_path, model_type='mpt')\n",
    "\n",
    "# Example usage (this part depends on your specific use case)\n",
    "# result = mpt.some_method()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac2af7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Ce PC\\\\Desktop\\\\GPT4ALL\\\\Nopenai\\\\GPT4ALL_env\\\\Lib\\\\site-packages\\\\~~t4all\\\\llmodel_DO_NOT_MODIFY\\\\build\\\\libgcc_s_seh-1.dll'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub gpt4all langchain-chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc29e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function LLModel._callback_decoder.<locals>._raw_callback at 0x000002317AA29630>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\_pyllmodel.py\", line 560, in _raw_callback\n",
      "    def _raw_callback(token_id: int, response: bytes) -> bool:\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "import os\n",
    "\n",
    "# Define the base path\n",
    "base_path = \"C:\\\\Users\\\\Ce PC\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\"\n",
    "# Define the model name\n",
    "model_name = \"mistral-7b-openorca.Q4_0.gguf\"\n",
    "# Combine the base path and model name to create the full path\n",
    "model_path = os.path.join(base_path, model_name)\n",
    "# Instantiate the model. Callbacks support token-wise streaming\n",
    "model = GPT4All(model=model_path, n_threads=8)\n",
    "\n",
    "# Generate text\n",
    "response = model.invoke(\"Once upon a time, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed43dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b2a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6ccac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367f8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the base path\n",
    "base_path = \"C:\\\\Users\\\\Ce PC\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\"\n",
    "# Define the model name\n",
    "model_name = \"mistral-7b-openorca.Q4_0.gguf\"\n",
    "# Combine the base path and model name to create the full path\n",
    "model_path = os.path.join(base_path, model_name)\n",
    "\n",
    "model = GPT4All(model=model_path, n_threads=8)\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5ad90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870380e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to find out when Justin Bieber was born. He was born on March 1, 1994. Now, let's see which NFL team won the Super Bowl in that year. The answer is the San Francisco 49ers who won the Super Bowl XXIX against the San Diego Chargers.\n",
      "\n",
      "Justin Bieber was born in London, Ontario, Canada on March 1, 1994. He is a famous Canadian singer and songwriter known for his pop music. His full name is Justin Drew Bieber. He started singing at a very young age and gained popularity using the internet to showcase his talent. In 2008, he was discovered by American manager Scooter Braun after watching his videos on YouTube.\n",
      "\n",
      "The San Francisco 49ers won Super Bowl XXIX in January 1995 against the San Diego Chargers with a score of 49-26. This victory marked their fifth Super Bowl win and was led by head coach George Seifert, quarterback Steve Young, and wide receiver Jerry Rice. The game took place at the Joe Robbie Stadium in Miami, Florida on January 29"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" First, we need to find out when Justin Bieber was born. He was born on March 1, 1994. Now, let's see which NFL team won the Super Bowl in that year. The answer is the San Francisco 49ers who won the Super Bowl XXIX against the San Diego Chargers.\\n\\nJustin Bieber was born in London, Ontario, Canada on March 1, 1994. He is a famous Canadian singer and songwriter known for his pop music. His full name is Justin Drew Bieber. He started singing at a very young age and gained popularity using the internet to showcase his talent. In 2008, he was discovered by American manager Scooter Braun after watching his videos on YouTube.\\n\\nThe San Francisco 49ers won Super Bowl XXIX in January 1995 against the San Diego Chargers with a score of 49-26. This victory marked their fifth Super Bowl win and was led by head coach George Seifert, quarterback Steve Young, and wide receiver Jerry Rice. The game took place at the Joe Robbie Stadium in Miami, Florida on January 29\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)\n",
    "\n",
    "# response = model.invoke(\"What NFL team won the Super Bowl in the year Justin Bieber was born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12df077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\" First, we need to find out when Justin Bieber was born. He was born on March 1, 1994. Now, let's see which NFL team won the Super Bowl in that year. The answer is the San Francisco 49ers who won the Super Bowl XXIX against the San Diego Chargers.\\n\\nJustin Bieber was born in London, Ontario, Canada on March 1, 1994. He is a famous Canadian singer and songwriter known for his pop music. His full name is Justin Drew Bieber. He started singing at a very young age and gained popularity using the internet to showcase his talent. In 2008, he was discovered by American manager Scooter Braun after watching his videos on YouTube.\\n\\nThe San Francisco 49ers won Super Bowl XXIX in January 1995 against the San Diego Chargers with a score of 49-26. This victory marked their fifth Super Bowl win and was led by head coach George Seifert, quarterback Steve Young, and wide receiver Jerry Rice. The game took place at the Joe Robbie Stadium in Miami, Florida on January 29\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3b076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT4ALL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
